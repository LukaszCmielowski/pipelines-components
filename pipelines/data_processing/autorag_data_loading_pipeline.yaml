# PIPELINE DEFINITION
# Name: autorag-data-processing-pipeline
# Description: Pipeline to load test data and documents for AutoRAG.
# Inputs:
#    input_data_bucket_name: str [Default: 'wnowogorski-test-bucket']
#    input_data_path: str [Default: '']
#    sampling_config: dict [Default: {}]
#    secret_name: str [Default: 'kubeflow-aws-secrets']
#    test_data_bucket_name: str [Default: 'wnowogorski-test-bucket']
#    test_data_path: str [Default: 'benchmark.json']
components:
  comp-document-loader:
    executorLabel: exec-document-loader
    inputDefinitions:
      artifacts:
        test_data:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: Optional artifact containing test data for sampling.
          isOptional: true
      parameters:
        input_data_bucket_name:
          parameterType: STRING
        input_data_path:
          parameterType: STRING
        sampling_config:
          defaultValue: {}
          description: Optional sampling configuration dictionary.
          isOptional: true
          parameterType: STRUCT
    outputDefinitions:
      artifacts:
        sampled_documents:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        Output:
          parameterType: STRING
  comp-test-data-loader:
    executorLabel: exec-test-data-loader
    inputDefinitions:
      parameters:
        test_data_bucket_name:
          parameterType: STRING
        test_data_path:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        test_data:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-text-extraction:
    executorLabel: exec-text-extraction
    inputDefinitions:
      artifacts:
        documents:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
          description: Input artifact containing the documents to process.
    outputDefinitions:
      artifacts:
        extracted_text:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        Output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-document-loader:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - document_loader
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'numpy' 'pandas'\
          \ 'boto3'  &&  python3 -m pip install --quiet --no-warn-script-location\
          \ 'kfp==2.15.2' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"\
          3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef document_loader(\n    input_data_bucket_name: str,\n    input_data_path:\
          \ str,\n    test_data: Input[Artifact] = None,\n    sampling_config: dict\
          \ = {},\n    sampled_documents: Output[Artifact] = None,\n) -> str:\n  \
          \  \"\"\"Document Loader component.\n\n    Loads documents from S3 and performs\
          \ sampling.\n\n    Args:\n        input_data_reference: Data source with\
          \ keys: connection_id, bucket, path.\n        test_data: Optional artifact\
          \ containing test data for sampling.\n        sampling_config: Optional\
          \ sampling configuration dictionary.\n        sampled_documents: Output\
          \ artifact containing sampled documents.\n\n    Returns:\n        Message\
          \ indicating completion status of document loading.\n    \"\"\"\n    import\
          \ os\n    import sys\n    import json\n    import logging\n\n    import\
          \ boto3\n\n\n    SUPPORTED_EXTENSIONS = {\".pdf\", \".docx\", \".pptx\"\
          , \".md\", \".html\", \".txt\"}\n    MAX_SIZE_BYTES = 1024 ** 3  # 1 GB\n\
          \n    logger = logging.getLogger(\"Document Loader component logger\")\n\
          \    logger.setLevel(logging.INFO)\n    if not logger.handlers:\n      \
          \  handler = logging.StreamHandler(sys.stdout)\n        logger.addHandler(handler)\n\
          \n    def get_test_data_docs_names(test_data: Input[Artifact]) -> list[str]:\n\
          \        if test_data is None:\n            return []\n        with open(test_data.path,\
          \ \"r\") as f:\n            benchmark = json.load(f)\n\n        docs_names\
          \ = []\n        for question in benchmark:\n            docs_names.extend(question[\"\
          correct_answer_document_ids\"])\n\n        return docs_names\n\n    def\
          \ download_docs_s3():\n        access_key = os.environ.get(\"AWS_ACCESS_KEY_ID\"\
          )\n        secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n    \
          \    endpoint_url = os.environ.get(\"AWS_ENDPOINT_URL\")\n        region\
          \ = os.environ.get(\"AWS_REGION\")\n\n        if (access_key and not secret_key)\
          \ or (secret_key and not access_key):\n            raise ValueError(\n \
          \               \"S3 credentials misconfigured: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\
          \ must either \"\n                \"both be set and non-empty, or both be\
          \ unset. Check the 's3-secret' Kubernetes secret.\"\n            )\n   \
          \     if not access_key and not secret_key:\n            raise ValueError(\n\
          \                \"S3 credentials missing: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\
          \ must be provided via \"\n                \"the 's3-secret' Kubernetes\
          \ secret when using s3:// dataset URIs.\"\n            )\n\n        s3_client\
          \ = boto3.client(\n            \"s3\",\n            endpoint_url=endpoint_url,\n\
          \            region_name=region,\n            aws_access_key_id=access_key,\n\
          \            aws_secret_access_key=secret_key,\n        )\n\n        contents\
          \ = s3_client.list_objects_v2(\n            Bucket=input_data_bucket_name,\n\
          \            Prefix=input_data_path,\n        ).get(\"Contents\", [])\n\n\
          \        supported_files = [\n            c for c in contents\n        \
          \    if c[\"Key\"].endswith(tuple(SUPPORTED_EXTENSIONS))\n        ]\n  \
          \      if not supported_files:\n            raise Exception(\"No supported\
          \ documents found.\")\n\n        test_data_docs_names = get_test_data_docs_names(test_data)\n\
          \n        supported_files.sort(key=lambda c: c[\"Key\"] not in test_data_docs_names)\n\
          \n        total_size = 0\n        documents_to_download = []\n\n       \
          \ for file in supported_files:\n            if total_size + file[\"Size\"\
          ] > MAX_SIZE_BYTES:\n                continue\n            documents_to_download.append(file)\n\
          \            total_size += file[\"Size\"]\n\n        os.makedirs(sampled_documents.path,\
          \ exist_ok=True)\n        for file_info in documents_to_download:\n    \
          \        key = file_info[\"Key\"]\n            safe_name = key.replace(\"\
          /\", \"__\")\n            local_path = os.path.join(sampled_documents.path,\
          \ safe_name)\n\n            try:\n                logger.info(f\"Downloading\
          \ {key} to {local_path}\")\n                s3_client.download_file(\n \
          \                   input_data_bucket_name,\n                    key,\n\
          \                    local_path\n                )\n            except Exception\
          \ as e:\n                logger.error(\"Failed to fetch %s: %s\", key, e)\n\
          \                raise\n\n    download_docs_s3()\n\n"
        image: python:3.11
    exec-test-data-loader:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - test_data_loader
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'boto3'  && \
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef test_data_loader(\n    test_data_bucket_name: str,\n    test_data_path:\
          \ str,\n    test_data: dsl.Output[dsl.Artifact] = None\n):\n    \"\"\"Test\
          \ Data Loader component.\n\n    TODO: Add a detailed description of what\
          \ this component does.\n\n    Args:\n        input_param: Description of\
          \ the component parameter.\n        # Add descriptions for other parameters\n\
          \n    Returns:\n        Description of what the component returns.\n   \
          \ \"\"\"\n    import os\n    import sys\n    import logging\n\n    import\
          \ boto3\n\n\n    logger = logging.getLogger(\"Test Data Loader component\
          \ logger\")\n    logger.setLevel(logging.INFO)\n    if not logger.handlers:\n\
          \        handler = logging.StreamHandler(sys.stdout)\n        logger.addHandler(handler)\n\
          \n    def get_test_data_s3():\n        access_key = os.environ.get(\"AWS_ACCESS_KEY_ID\"\
          )\n        secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n    \
          \    endpoint_url = os.environ.get(\"AWS_ENDPOINT_URL\")\n        region\
          \ = os.environ.get(\"AWS_REGION\")\n\n        if (access_key and not secret_key)\
          \ or (secret_key and not access_key):\n            raise ValueError(\n \
          \               \"S3 credentials misconfigured: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\
          \ must either \"\n                \"both be set and non-empty, or both be\
          \ unset. Check the 's3-secret' Kubernetes secret.\"\n            )\n   \
          \     if not access_key and not secret_key:\n            raise ValueError(\n\
          \                \"S3 credentials missing: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\
          \ must be provided via \"\n                \"the 's3-secret' Kubernetes\
          \ secret when using s3:// dataset URIs.\"\n            )\n\n        s3_client\
          \ = boto3.client(\n            \"s3\",\n            endpoint_url=endpoint_url,\n\
          \            region_name=region,\n            aws_access_key_id=access_key,\n\
          \            aws_secret_access_key=secret_key,\n        )\n\n        if\
          \ test_data_path.path.endswith(\".json\"):\n            logger.info(f\"\
          Fetching test data from S3: bucket={test_data_bucket_name}, path={test_data_path}\"\
          )\n            try:\n                logger.info(f\"Starting download to\
          \ {test_data.path}\")\n                s3_client.download_file(\n      \
          \              test_data_bucket_name,\n                    test_data_path,\n\
          \                    test_data.path\n                )\n               \
          \ logger.info(\"Download completed successfully\")\n            except Exception\
          \ as e:\n                logger.error(\"Failed to fetch %s: %s\", test_data_path,\
          \ e)\n                raise\n        else:\n            logger.error(\"\
          Test data must be a json file: %s\", test_data_path)\n            raise\n\
          \n    get_test_data_s3()\n\n"
        image: python:3.11
    exec-text-extraction:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - text_extraction
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.15.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef text_extraction(\n    documents: Input[Artifact],\n    extracted_text:\
          \ Output[Artifact],\n) -> str:\n    \"\"\"Text Extraction component.\n\n\
          \    Extracts text from provided documents (PDF, DOCX, PPTX, MD, HTML, TXT)\
          \ using the docling library.\n\n    Args:\n        documents: Input artifact\
          \ containing the documents to process.\n        extracted_text: Output artifact\
          \ where the extracted text content will be stored.\n\n    Returns:\n   \
          \     A message indicating the completion status and processing statistics.\n\
          \    \"\"\"\n    import os\n    import logging\n    import sys\n    from\
          \ pathlib import Path\n    from concurrent.futures import ThreadPoolExecutor\n\
          \n    from docling.document_converter import DocumentConverter, PdfFormatOption\n\
          \    from docling.datamodel.base_models import InputFormat\n    from docling.datamodel.pipeline_options\
          \ import PdfPipelineOptions\n\n    logger = logging.getLogger(\"Text Extraction\
          \ component logger\")\n    logger.setLevel(logging.INFO)\n    if not logger.handlers:\n\
          \        handler = logging.StreamHandler(sys.stdout)\n        logger.addHandler(handler)\n\
          \n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.do_ocr\
          \ = False\n    pipeline_options.do_table_structure = True\n\n    converter\
          \ = DocumentConverter(\n        format_options={\n            InputFormat.PDF:\
          \ PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\
          \n    input_dir = Path(documents.path)\n    output_dir = Path(extracted_text.path)\n\
          \    output_dir.mkdir(parents=True, exist_ok=True)\n\n    SUPPORTED_EXTENSIONS\
          \ = {\".pdf\", \".docx\", \".pptx\", \".md\", \".html\", \".txt\"}\n\n \
          \   if not input_dir.exists():\n        msg = f\"Input directory {input_dir}\
          \ does not exist.\"\n        logger.error(msg)\n        return msg\n\n \
          \   files_to_process = [\n        f for f in input_dir.iterdir() \n    \
          \    if f.is_file() and f.suffix.lower() in SUPPORTED_EXTENSIONS\n    ]\n\
          \n    logger.info(f\"Starting text extraction for {len(files_to_process)}\
          \ documents.\")\n\n    def process_file(file_path: Path):\n        try:\n\
          \            logger.info(f\"Processing document: {file_path.name}\")\n\n\
          \            result = converter.convert(file_path)\n\n            markdown_content\
          \ = result.document.export_to_markdown()\n\n            output_file_name\
          \ = f\"{file_path.stem}.md\"\n            output_file_path = output_dir\
          \ / output_file_name\n\n            with open(output_file_path, \"w\", encoding=\"\
          utf-8\") as f:\n                f.write(markdown_content)\n\n          \
          \  logger.info(f\"Successfully extracted text from {file_path.name}\")\n\
          \            return True\n        except Exception as e:\n            logger.error(f\"\
          Failed to process {file_path.name}: {str(e)}\")\n            return False\n\
          \n    max_workers = min(len(files_to_process), (os.cpu_count() or 1) * 2)\
          \ if files_to_process else 1\n\n    with ThreadPoolExecutor(max_workers=max_workers)\
          \ as executor:\n        results = list(executor.map(process_file, files_to_process))\n\
          \n    processed_count = sum(1 for r in results if r)\n    error_count =\
          \ len(results) - processed_count\n\n    summary = f\"Text extraction completed.\
          \ Total processed: {processed_count}, Errors: {error_count}.\"\n    logger.info(summary)\n\
          \n"
        image: quay.io/openshift_trial/custom_images:docling
pipelineInfo:
  description: Pipeline to load test data and documents for AutoRAG.
  name: autorag-data-processing-pipeline
root:
  dag:
    tasks:
      document-loader:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-document-loader
        dependentTasks:
        - test-data-loader
        inputs:
          artifacts:
            test_data:
              taskOutputArtifact:
                outputArtifactKey: test_data
                producerTask: test-data-loader
          parameters:
            input_data_bucket_name:
              componentInputParameter: input_data_bucket_name
            input_data_path:
              componentInputParameter: input_data_path
            sampling_config:
              componentInputParameter: sampling_config
        taskInfo:
          name: document-loader
      test-data-loader:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-test-data-loader
        inputs:
          parameters:
            test_data_bucket_name:
              componentInputParameter: test_data_bucket_name
            test_data_path:
              componentInputParameter: test_data_path
        taskInfo:
          name: test-data-loader
      text-extraction:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-text-extraction
        dependentTasks:
        - document-loader
        inputs:
          artifacts:
            documents:
              taskOutputArtifact:
                outputArtifactKey: sampled_documents
                producerTask: document-loader
        taskInfo:
          name: text-extraction
  inputDefinitions:
    parameters:
      input_data_bucket_name:
        defaultValue: wnowogorski-test-bucket
        isOptional: true
        parameterType: STRING
      input_data_path:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      sampling_config:
        defaultValue: {}
        isOptional: true
        parameterType: STRUCT
      secret_name:
        defaultValue: kubeflow-aws-secrets
        isOptional: true
        parameterType: STRING
      test_data_bucket_name:
        defaultValue: wnowogorski-test-bucket
        isOptional: true
        parameterType: STRING
      test_data_path:
        defaultValue: benchmark.json
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.15.2
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-document-loader:
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: aws_access_key_id
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: aws_secret_access_key
            - envVar: AWS_ENDPOINT_URL
              secretKey: endpoint_url
            - envVar: AWS_REGION
              secretKey: aws_region_name
            optional: false
            secretNameParameter:
              componentInputParameter: secret_name
        exec-test-data-loader:
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: aws_access_key_id
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: aws_secret_access_key
            - envVar: AWS_ENDPOINT_URL
              secretKey: endpoint_url
            - envVar: AWS_REGION
              secretKey: aws_region_name
            optional: false
            secretNameParameter:
              componentInputParameter: secret_name
